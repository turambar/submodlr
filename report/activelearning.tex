\subsection{Supervised learning theory}

In classic supervised machine learning, the learning algorithm (or \textit{learner}) is given the task of finding a response function $f: \mathcal{X} \mapsto \mathcal{Y}$ that predicts as accurately as possible the output \textit{response} $Y \in \mathcal{Y}$ for a given input observation	 $X \in \mathcal{X}$ \cite{Mohri:2012}. Responses take a variety of forms. In classification, this may be a label from a discrete set of choices $\mathcal{Y} = \{ 1, 2, \dots\}$, while in regression it may be continuous. One of the most common tasks is binary classification, in which $\mathcal{Y} = \pm1$. We have some unknown underlying distribution $\mathcal{D}$ over the space of observations and responses $\mathcal{X} \times \mathcal{Y}$, so that observation-response pairs are sampled according to $(X, Y) \thicksim \mathcal{D}$. The learner chooses from candidate functions or \textit{hypotheses} in a hypothesis space $\mathcal{H}$ with the goal of minimizing the expected error or \textit{risk} $\epsilon_\mathcal{D}(h) = \mathbf{E}_{(X,Y)\thicksim \mathcal{D}}[\mathrm{err}(h(X), Y)]$. In other words, the learner's goal is to find $h^\ast$ that minimizes the risk: $h^\ast = \arg\max_{h \in \mathcal{H}} \epsilon_{\mathcal{D}}(h)$. For standard classification tasks, the error function is simply the indicator function of a mistake $\mathds{1}\{h(X) \not= Y\}$, and so the risk is simply the probability of a mistake $\epsilon(h) = \mathbf{E}_{(X,Y)\thicksim \mathcal{D}}[\mathds{1}\{h(X) \not= Y\}] = \mathbf{Pr}\{h(X) \not= Y\}$. For continuous response functions and multiclass classification where order matters, there are a wide choice of more complex error functions.

Of course, in practice $\mathcal{D}$ is unknown and so it is impossible to directly minimize the risk. Instead, the learner is provided with ``supervision'' in the form of a finite sample of observation-response pairs, i.e., a labeled \textit{training} data set $\mathcal{S} = \{ X_i, Y_i \}_{i=1, \dots, n}$ where $|\mathcal{S}| = n$. The learner can then approximate $\mathcal{D}$ using $\mathcal{S}$ and minimize the empirical error over $\mathcal{S}$:
\[
\hat{\epsilon}_{\mathcal{S}}(h) = \mathbf{E}_{(X,Y) \in \mathcal{S}}[\mathrm{err}(h(X), Y)] = \frac{1}{n} \sum_{i=1}^n \mathrm{err}(h(X_i), Y_i)
\]

\noindent Note that this definition of empirical risk assumes that samples $(X, Y)$ are identically independently distributed (IID), a fairly common assumption in supervised machine learning. In the \textit{empirical risk minimization} (ERM) paradigm, the learner assumes that the sample $\mathcal{S}$ is sufficiently representative of $\mathcal{D}$ such that choosing $\hat{h} = \arg\max_{h \in \mathcal{H}} \hat{\epsilon}_{\mathcal{S}}$ will yield a hypothesis $\hat{h}$ that will also have a relatively low risk $\epsilon_{\mathcal{D}}(\hat{h})$ \cite{Vapnik:2000}. A well known theoretical result for classification that comes from Vapnik tells us if we want to learn a ``good'' classifier from a hypothesis class $\mathcal{H}$, then we need roughly $|\mathcal{S}| = \widetilde{O}\left(d/\varepsilon^2 \log (1/\delta)\right)$ points in our training sample \cite{Vapnik:1999}. Here $\varepsilon$ is the maximum deviation that we will tolerate between the true risks of $\hat{h}$ and optimal $h^\ast$ and $\delta$ is the probability with which we are willing to let this happen (i.e., we want $|\epsilon(\hat{h}) - \epsilon(h^\ast)| \leq \varepsilon$ to hold with probability $1-\delta$). Informally, $d$ represents the ``size'' of our hypothesis class; formally, it is the \textit{VC dimension}. A useful rule of thumb is that for most useful hypothesis classes, the VC dimension scales linearly with the number of parameters and so the number of training samples needed scales linearly with ``complexity'' of the model.

It is important to distinguish two cases of supervised learning, based on realizability. When the problem is \textit{realizable}, then there exists some hypothesis $h \in \mathcal{H}$ that can perfectly predict the response for every point (i.e., $\mathrm{err}(h^\ast) = 0$); in binary classification, this corresponds to the problem being ``separable'' by a hypothesis in $\mathcal{H}$. When $\mathrm{err}(h) > 0$, the problem is not realizable \cite{Dasgupta:2011}. The presence of \textit{label noise}, where the same point may receive different responses, further complicates this picture. If a training sample $\mathcal{S}$ contains noisy labels (perhaps due to error), this may mislead the ERM. If the true data distribution allows points to have different labels (i.e., our true labeling function is stochastic), then at best we may only be able to model $P(Y|X)$, rather than make perfect predictions.

\subsection{Selective sampling as a submodular problem}

Imagine the following problem, which we will call the \textbf{selective sampling on a budget} problem: given a large, fully labeled finite sample $\mathcal{S}$, we will ``purchase'' a subset $\mathcal{L} \subseteq \mathcal{S}$ (where $|\mathcal{L}| \ll |\mathcal{S}|$ because our ``cost'' scales with $|\mathcal{L}|$) and train $\bar{h} = \arg\max_{h \in \mathcal{H}} \hat{\epsilon}_{\mathcal{L}}(h)$ with the goal of minimizing $\epsilon_{\mathcal{D}}(\bar{h})$. We are given full access to $\mathcal{S}$ until we make our purchase decision, at which point we can use \textit{only} $\mathcal{L}$ to choose our final hypothesis (i.e., we must ``forget'' everything we know about $\mathcal{S} \setminus \mathcal{L}$). This can be thought of as choosing the smallest possible representative subsample $\mathcal{L}$. Intuitively, it is similar to a set cover problem: we want to pose queries that ``cover'' (i.e., eliminate) as many false hypotheses (inconsistent with our labeled data set) as possible. This problem clearly has submodular structure.

\begin{lemma}\label{lma:ss}
The \textbf{selective sampling on a budget} problem is submodular and monotone decreasing.
\end{lemma}

\begin{proof}
We provide a non-rigorous justification. First, for labeled subsample $\mathcal{A}$, define a hypothesis set $\mathcal{H}_{\mathcal{A}} \subseteq \mathcal{H}$ that contains all hypotheses from $\mathcal{H}$ that are consistent with the labeled points in $\mathcal{A}$: $\mathcal{H}_{\mathcal{A}} = \{h : \hat{\epsilon}_{\mathcal{A}}(h)=0 \wedge h \in \mathcal{H}\}$. Now define a function $f(\mathcal{A}) = 1-|\mathcal{H}_{\mathcal{A}}| / |\mathcal{H}|$, i.e., maps $\mathcal{A}$ to the value of 1 minus probability mass (under a uniform prior) of its consistent hypothesis set. Now consider labeled subsamples $\mathcal{B}$ and $\mathcal{B}'$ such that $\mathcal{B} \subseteq \mathcal{B}' \subseteq \mathcal{S}$ and arbitrary point $X \not\in \mathcal{B}, \not\in \mathcal{B}'$. The key insight here is that as we add labeled points to our subsamples, we can only remove hypotheses from our current hypothesis space; once a hypothesis has been removed, it cannot be re-added.

\textbf{$f$ is monotone increasing:} Suppose that hypothesis $h \in \mathcal{H}_{\mathcal{B}'}$. This means that it is consistent with every labeled point in $\mathcal{B}'$. Because $\mathcal{B} \subseteq \mathcal{B}'$, $h$ must also be consistent with every point in $\mathcal{B}$ and so $h \in \mathcal{H}_{\mathcal{B}}$. Therefore, $\mathcal{H}_{\mathcal{B}'} \subseteq \mathcal{H}_{\mathcal{B}}$ and so

\begin{eqnarray*}
|\mathcal{H}_{\mathcal{B}'}| &\leq& |\mathcal{H}_{\mathcal{B}}| \\
|\mathcal{H}_{\mathcal{B}'}| / |\mathcal{H}| &\leq& |\mathcal{H}_{\mathcal{B}}| / |\mathcal{H}| \\
1-|\mathcal{H}_{\mathcal{B}'}| / |\mathcal{H}| &\geq& 1-|\mathcal{H}_{\mathcal{B}}| / |\mathcal{H}| \\
f(\mathcal{B}') &\geq& f(\mathcal{B})
\end{eqnarray*}

\noindent whenever $\mathcal{B} \subseteq \mathcal{B}'$.

\textbf{$f$ is submodular:} Now suppose that adding point $X$ to $\mathcal{B}'$ removes $h$ from $\mathcal{H}_{\mathcal{B}'}$, i.e., $h \in \mathcal{H}_{\mathcal{B}'} \setminus \mathcal{H}_{\mathcal{B}' \cup \{X\}}$. Because $\mathcal{H}_{\mathcal{B}'} \subseteq \mathcal{H}_{\mathcal{B}}$ whenever $\mathcal{B} \subseteq \mathcal{B}'$, it must also be the case that $\mathcal{H}_{\mathcal{B}' \cup \{X\}} \subseteq \mathcal{H}_{\mathcal{B} \cup \{X\}}$ and so $h \in \mathcal{H}_{\mathcal{B}} \setminus \mathcal{H}_{\mathcal{B} \cup \{X\}}$. Thus, if adding $X$ to $\mathcal{B}'$ removes $m$ hypotheses from $\mathcal{H}_{\mathcal{B}'}$, then adding $X$ to $\mathcal{B}$ must remove $n \geq m$ hypotheses from $\mathcal{H}_{\mathcal{B}}$.

\begin{eqnarray*}
m &\leq& n \\
|\mathcal{H}_{\mathcal{B}'}| - |\mathcal{H}_{\mathcal{B}'}| + m &\leq& |\mathcal{H}_{\mathcal{B}}| - |\mathcal{H}_{\mathcal{B}}| + n \\
|\mathcal{H}_{\mathcal{B}'}|/|\mathcal{H}| - (|\mathcal{H}_{\mathcal{B}'}| - m)/|\mathcal{H}| &\leq& |\mathcal{H}_{\mathcal{B}}|/|\mathcal{H}| - (|\mathcal{H}_{\mathcal{B}}| - n)/|\mathcal{H}| \\
-1 + |\mathcal{H}_{\mathcal{B}'}|/|\mathcal{H}| + 1 - (|\mathcal{H}_{\mathcal{B}'}| - m)/|\mathcal{H}| &\leq& -1 + |\mathcal{H}_{\mathcal{B}}|/|\mathcal{H}| + 1 - (|\mathcal{H}_{\mathcal{B}}| - n)/|\mathcal{H}| \\
1 - (|\mathcal{H}_{\mathcal{B}'}| - m)/|\mathcal{H}| - (1 - |\mathcal{H}_{\mathcal{B}'}|/|\mathcal{H}|) &\leq& 1 - (|\mathcal{H}_{\mathcal{B}}| - n)/|\mathcal{H}| - (1 - |\mathcal{H}_{\mathcal{B}}|/|\mathcal{H}|) \\
f(\mathcal{B}' \cup \{X\}) - f(\mathcal{B}') &\leq& f(\mathcal{B} \cup \{X\}) - f(\mathcal{B})
\end{eqnarray*}

\noindent whenever $\mathcal{B} \subseteq \mathcal{B}'$.
\end{proof}
\noindent This is intuitive. If $\mathcal{H}' \subseteq \mathcal{H}$ contains all hypotheses in $\mathcal{H}$ that are inconsistent with $X$'s label, then clearly $\mathcal{H}_{\mathcal{B}'} \subseteq \mathcal{H}_{\mathcal{B}}$ implies that $\mathcal{H}_{\mathcal{B}'} \setminus \mathcal{C} \subseteq \mathcal{H}_{\mathcal{B}} \setminus \mathcal{C}$.

This result may not seem terribly exciting, but what it does suggest is that we can solve the selective sampling on a budget problem using a greedy approach: on the $t$th iteration, choose the $X$ that eliminates the largest number of inconsistent hypotheses from our current $\mathcal{H}_{\mathcal{B}}$:
\[
(X, Y)_t = \underset{(X,Y) \in \mathcal{S} \setminus \mathcal{B}_t}{\arg\max} \left|\sum_{h \in \mathcal{H}_{\mathcal{B}_t}} \mathds{1}\{h(X) \not= Y\} \right|
\]

\subsection{Greedy active learning is adaptive submodular}

Now imagine a variation of the above selective sampling problem where we do not have access to the label of $X \in \mathcal{S}$ until we ``purchase'' it. Here we might use \textit{active learning}. Active learning is a variation of the supervised learning paradigm where the learner does not receive access to a fully labeled data sample $\mathcal{S}$ upfront. Rather it has access to an unlabeled data sample $\mathcal{U} = \{(X, ?)\}$, as well as an \textit{oracle} that the learner can \textit{query} for the response (or label) of an observation, $Y = \mathrm{or}(X)$ \cite{Dasgupta:2011}. The active learner is given agency to choose which individual samples to label, but each query has a cost $c$ and the learner has only a limited \textit{budget} to spend on labeling data. Similar to the selective sampling scenario described above, the active learner has dual goals: to choose simultaneously a labeled subset of observations $\mathcal{L} \subseteq \mathcal{U}$ and a hypothesis $\bar{h} = \arg\min_{h \in \mathcal{H}} \hat{\epsilon}_{\mathcal{L}}(h)$ (i.e., $\bar{h}$ is the ERM for $\mathcal{L}$) that will yield the best possible predictive performance (i.e., lowest risk $\epsilon_{\mathcal{D}}(\bar{h}))$.

When evaluating active learning algorithms, we are concerned primarily with two performance properties: the quality (in terms of risk) of the hypotheses they produce and their query efficiency. Intuitively, a good active learner will use a very small number of label queries to produce a hypothesis with very small predictive error. More formally, we are interested in (1) how the error of the hypothesis produced by an active learner that chooses labeled subsample $\mathcal{L}$ compares with that of the hypothesis that we could learn from a fully labeled sample $\mathcal{S}$ where $\mathcal{L} \subseteq \mathcal{S}$; and (2) how many label queries must be made to achieve a certain level of performance, which we call \textit{label complexity} and express in \textit{Big-Oh} notation. An ideal active learner will compete with fully supervised learning with $|\mathcal{L}| \ll |\mathcal{S}|$. More realistically, we hope to at least place an upper bound on the error of active learning that is within a constant (multiplicative or additive) factor of the error of fully labeled supervised learning.

It is not hard to design greedy approaches to active learning, but two questions arise: first, are there greedy active learning algorithms that have sound theoretical guarantees about error and label complexity; and second, can we show that such algorithms are in fact specific cases of more general approaches based on submodularity? The answer to both of these questions is, in fact, yes \cite{Dasgupta:2004}. Let $\mathcal{L}_t$ be the set of labeled data points after $t$ queries (and recall that $\mathcal{H}_{\mathcal{L}_t}$ is the set of hypotheses from $\mathcal{H}$ consistent with the labeled data in $\mathcal{L}_t$). For the next ($t+1$) label query, we want to choose the unlabeled point that provokes the greatest disagreement between hypotheses in $\mathcal{H}_{\mathcal{L}_t}$.The maximum disagreement occurs when half of the hypotheses predict one label and the rest the other. Equivalently, this minimizes the absolute value of the sum of all predicted labels: $| \sum_{h \in \mathcal{H}_{\mathcal{L}_t}} h(x)|$ (when using $\pm1$ labels). Following this query policy, we hope to cut the $t$th hypothesis space roughly in half with query $t+1$ and achieve a label complexity that is roughly $O(\log (d/\varepsilon))$ for $d$ the size (e.g., VC dimension) of the hypothesis space. \cite{Dasgupta:2004} shows that in the worst case, this strategy may have to query every single label; indeed, for certain pathological cases, even the optimal query strategy will need to query ever label. However, the average case analysis is much more promising. On average, the greedy strategy's label complexity is at most $\widetilde{O}(\log d)$ times larger that of the optimal policy, as we show below in \textbf{Theorem \ref{thm:dasgupta}}, which rephrases \textit{Claim 4} and \textit{Theorem 3} from \cite{Dasgupta:2004}:\\

\begin{theorem}[Dasgupta ~\cite{Dasgupta:2004}]
\label{thm:dasgupta}
Suppose the optimal query policy requires $M$ labels in expectation for target hypotheses chosen uniformly from hypothesis class $\mathcal{H}$ of (VC) dimension $d \geq e^e \approx 16$. Then the expected number of labels queried by the greedy strategy is at least $\frac{m \log d}{\log \log d}$ and at most $4 M \log d$.
\end{theorem}

As \cite{Dasgupta:2004} points out, the lower bound is a bit depressing, but we derive some comfort from the fact that the upper bound matches the lower bound within a multiplicative factor. We can extend this analysis to a Bayesian framework where we have a prior distribution over hypotheses $\pi(h)$. In this setting, we seek a label query that will divide the \textit{probability mass} over hypotheses (rather than the hypothesis space itself) in half. We do this by minimizing the absolute value of the sum of predictions weighted by the prior probabilities of the hypotheses making them: $| \sum_{h \in \mathcal{H}_t} \pi(x) h(x)|$. In this case, the $d$ term in the lower and upper bounds is replaced with $\min_{h \in \mathcal{H}} \pi(h)$.

In \cite{Golovin}, the authors show that the hypothesis space reduction problem is adaptive submodular, specifically an example of an adaptive stochastic coverage problem. Here our ground set is the set of all points $V = \{x : x \in \mathcal{U}\}$, and each point has an unobserved state $O = \{y : y \in \mathcal{Y}|\} = \{\pm1\}$ where $\Phi(x) = y$ for the pair $(x,y)$ and for a given hypothesis $h \in \mathcal{H}$, $\Phi_h(x) = h(x)$. The set of labeled points $\mathcal{L}_t$ forms a consistent partial realization $\chi_t$ at iteration $t$. Then we can define a function that takes as input an element subset $V'$ and a realization function $\Phi'$ and maps it to a real number in the interval $[0,1]$:
\[
f(V', \Phi') = \hat{f}(H = \{h : h(x) = \Phi'(x) \mbox{ for all } x \in V'\}) = 1 - \sum_{h \in H} \pi(h)
\]

\noindent So for a labeled subset $\mathcal{L}_t$, $f(V_t, \Phi_t) = \hat{f}(\mathcal{L}_t)$, where $V_t = \{ x : (x,y) \in \mathcal{L}_t \}$ and $\Phi_t(x) = \Psi_t(x) = y$ for $(x, y) \in \mathcal{L}_t$. This function is adaptively submodular, as shown in \textbf{Lemma \ref{lma:gbs}}, adapted from \cite{Golovin}:\\

\begin{lemma}[Golovin and Krause ~\cite{Golovin}]
\label{lma:gbs}
The \textbf{hypothesis space reduction} problem is adaptive submodular and adaptive monotone.
\end{lemma}

\noindent The proof of monotonicity follows along lines similar to the one used in \textbf{Theorem \ref{lma:ss}}: basically, querying a label can only remove hypotheses, and hypothesis probabilities are nonzero, so removing one can only reduce the value of $f$. The proof of submodularity is more subtle, though it rests on the same intuition as that of monotonicity and involves comparing the conditional expected marginal benefits, as described in. Interestingly, this angle yields a slightly more optimistic average case analysis than that given in \textbf{Theorem \ref{thm:dasgupta}} above, removing the constant multiplier from the upper bound. We give it below in \textbf{Theorem \ref{thm:golovin}}, adapted from \cite{Golovin}:\\

\begin{theorem}[Golovin and Krause ~\cite{Golovin}]
\label{thm:golovin}
Suppose the optimal query policy requires $M$ labels in expectation for target hypotheses chosen using distribution $\pi$ from hypothesis class $\mathcal{H}$. Then the expected number of labels queried by the greedy strategy is at most $M \left(\log \left(\frac{1}{\min_{h \in \mathcal{H}} \pi(h)}\right) + 1\right)$.
\end{theorem}

\subsection{New directions}

\noindent This is a wonderful example of cross fertilization between research in computer science theory and optimization and learning theory. Working on submodularity and adaptive submodularity, computer scientists were able to rediscover and generalize previously published results from machine learning, improving an upper bound along the way. More important, they provided new and useful insights into the problem, relating it to other problems (which we did not discuss in this section) and paving the way to new discoveries. Recently, there has been an explosion of similar work, much of published in 2013. \cite{mirzasoleiman13distributed} describe a framework for performing distributed submodular maximization in a shared-nothing (MapReduce) storage setting and using it to choose a representative subsample of a massive data set for learning (similar to our selective sampling on a budget problem). \cite{chen13near} describe a greedy \textit{batch-mode} active learning algorithm that queries labels in batches of size $k > 1$ and show that this approach is competitive not only with optimal batch-more active learning but also with more traditional greedy active learning. There are a variety of other papers pushing the boundary in this area \cite{GolovinK11} \cite{golovin10near} \cite{zuluaga13active} \cite{hollinger:2011}.

There are two lines of work that seem conspicuously absent (at least, based on our admittedly myopic literature review): (1) applications of submodularity to \textit{streaming active learning}; and (2) ``aggressive'' active learning in the nonrealizable case. The former involves active learning when we do \textit{not} have access to the entirety of $\mathcal{U}$ at the start of the learning process. Rather, we receive one data point at a time in an online fashion and must make a query decision for point $X_t$ based only on the samples $\mathcal{U}_t$ that we've seen so far. The above greedy algorithm and analysis require that we be able to choose a point $X_t = \arg\min_{X \in \mathcal{U} \setminus \mathcal{L}_t} \left| \sum_{h \in \mathcal{H}_{\mathcal{L}_t}} \pi(h) h(X) \right|$. We cannot, of course, do this in the streaming setting. Nonetheless, intuition suggests that we should be able to extend the adaptive submodularity framework to this setting.

Aggressive active learning in the non-realizable case is a wide open problem, at least as of \cite{Dasgupta:2011}. Informally, aggressive active learners, which include greedy active learners, are those that attempt to make the ``most informative'' label query at each step. In the realizable case, it is possible to develop aggressive algorithms that are statistically consistent (will discover the optimal hypothesis with enough queries) and have sound theoretical guarantees for label complexity. However, these guarantees go out the window with realizability. Perhaps submodularity may help here, although naively, it looks as though the nonrealizable case will not satisfy the assumptions necessary for submodularity. We now have a variety of mellow active learners, which seek \textit{any} informative label query, that are label efficient and statistically sound. It would be interesting to develop a new analysis of these algorithms in terms of submodularity and then to see if this analysis perhaps provides a bridge between mellow and aggressive active learning.


%Some interesting and recent examples include Bayesian experimental design and active learning with noisy labels
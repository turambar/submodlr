\subsection{Supervised learning theory}

In classic supervised machine learning, the learning algorithm (or \textit{learner}) is given the task of finding a response function $f: \mathcal{X} \mapsto \mathcal{Y}$ that predicts as accurately as possible the output \textit{response} $Y \in \mathcal{Y}$ for a given input observation	 $X \in \mathcal{X}$. Responses take a variety of forms. In classification, this may be a label from a discrete set of choices $\mathcal{Y} = \{ 1, 2, \dots\}$, while in regression it may be continuous. One of the most common tasks is binary classification, in which $\mathcal{Y} = \pm1$. We have some unknown underlying distribution $\mathcal{D}$ over the space of observations and responses $\mathcal{X} \times \mathcal{Y}$, so that observation-response pairs are sampled according to $(X, Y) \thicksim \mathcal{D}$. The learner chooses from candidate functions or \textit{hypotheses} in a hypothesis space $\mathcal{H}$ with the goal of minimizing the expected error or \textit{risk} $\epsilon_\mathcal{D}(h) = \mathbf{E}_{(X,Y)\thicksim \mathcal{D}}[\mathrm{err}(h(X), Y)]$. In other words, the learner's goal is to find $h^\ast$ that minimizes the risk: $h^\ast = \arg\max_{h \in \mathcal{H}} \epsilon_{\mathcal{D}}(h)$. For standard classification tasks, the error function is simply the indicator function of a mistake $\mathds{1}\{h(X) \not= Y\}$, and so the risk is simply the probability of a mistake $\epsilon(h) = \mathbf{E}_{(X,Y)\thicksim \mathcal{D}}[\mathds{1}\{h(X) \not= Y\}] = \mathbf{Pr}\{h(X) \not= Y\}$. For continuous response functions and multiclass classification where order matters, there are a wide choice of more complex error functions.

Of course, in practice $\mathcal{D}$ is unknown and so it is impossible to directly minimize the risk. Instead, the learner is provided with ``supervision'' in the form of a finite sample of observation-response pairs, i.e., a labeled \textit{training} data set $\mathcal{S} = \{ X_i, Y_i \}_{i=1, \dots, n}$ where $|\mathcal{S}| = n$. The learner can then approximate $\mathcal{D}$ using $\mathcal{S}$ and minimize the empirical error over $\mathcal{S}$:
\[
\hat{\epsilon}_{\mathcal{S}}(h) = \mathbf{E}_{(X,Y) \in \mathcal{S}}[\mathrm{err}(h(X), Y)] = \frac{1}{n} \sum_{i=1}^n \mathrm{err}(h(X_i), Y_i)
\]

\noindent Note that this definition of empirical risk assumes that samples $(X, Y)$ are identically independently distributed (IID), a fairly common assumption in supervised machine learning. In the \textit{empirical risk minimization} (ERM) paradigm, the learner assumes that the sample $\mathcal{S}$ is sufficiently representative of $\mathcal{D}$ such that choosing $\hat{h} = \arg\max_{h \in \mathcal{H}} \hat{\epsilon}_{\mathcal{S}}$ will yield a hypothesis $\hat{h}$ that will also have a relatively low risk $\epsilon_{\mathcal{D}}(\hat{h})$. A well known theoretical result for classification that comes from Vapnik tells us if we want to learn a ``good'' classifier from a hypothesis class $\mathcal{H}$, then we need roughly $|\mathcal{S}| = \widetilde{O}\left(d/\varepsilon^2 \log (1/\delta)\right)$ points in our training sample. Here $\varepsilon$ is the maximum deviation that we will tolerate between the true risks of $\hat{h}$ and optimal $h^\ast$ and $\delta$ is the probability with which we are willing to let this happen (i.e., we want $|\epsilon(\hat{h}) - \epsilon_(h^\ast) \leq \varepsilon$ to hold with probability $1-\delta$). Informally, $d$ represents the ``size'' of our hypothesis class; formally, it is the \textit{VC dimension}. A useful rule of thumb is that for most useful hypothesis classes, the VC dimension scales linearly with the number of parameters and so the number of training samples needed scales linearly with ``complexity'' of the model.

It is important to distinguish two cases of supervised learning, based on realizability. When the problem is \textit{realizable}, then there exists some hypothesis $h \in \mathcal{H}$ that can perfectly predict the response for every point (i.e., $\mathrm{err}(h^\ast) = 0$); in binary classification, this corresponds to the problem being ``separable'' by a hypothesis in $\mathcal{H}$. When $\mathrm{err}(h) > 0$, the problem is not realizable. The presence of \textit{label noise}, where the same point may receive different responses, further complicates this picture. If a training sample $\mathcal{S}$ contains noisy labels (perhaps due to error), this may mislead the ERM. If the true data distribution allows points to have different labels (i.e., our true labeling function is stochastic), then at best we may only be able to model $P(Y|X)$, rather than make perfect predictions.

\subsection{Active learning}

\textit{Active learning} is a variation of the supervised learning paradigm where the learner does not receive access to a fully labeled data sample $\mathcal{S}$ upfront. Rather it has access to an unlabeled data sample $\mathcal{U} = \{(X, ?)\}$, as well as an \textit{oracle} that the learner can \textit{query} for the response (or label) of an observation, $Y = \mathrm{or}(X)$. The active learner is given agency to choose which individual samples to label, but each query has a cost $c$ and the learner has only a limited \textit{budget} to spend on labeling data. Thus, the active learner has dual goals that generalize the goal of the ``passive'' supervised learner: to choose simultaneously a labeled subset of observations $\mathcal{L} \subseteq \mathcal{U}$ and a hypothesis $\bar{h} = \arg\min_{h \in \mathcal{H}} \hat{\epsilon}_{\mathcal{L}}(h)$ (i.e., $\bar{h}$ is the ERM for $\mathcal{L}$) that will yield the best possible predictive performance (i.e., lowest risk $\epsilon_{\mathcal{D}}(\bar{h})$.

When evaluating active learning algorithms, we are concerned primarily with two performance properties: the quality (in terms of risk) of the hypotheses they produce and their query efficiency. Intuitively, a good active learner will use a very small number of label queries to produce a hypothesis with very small predictive error. More formally, we are interested in (1) how the error of the hypothesis produced by an active learner that chooses labeled subsample $\mathcal{L}$ compares with that of the hypothesis that we could learn from a fully labeled sample $\mathcal{S}$ where $\mathcal{L} \subseteq \mathcal{S}$; and (2) how many label queries must be made to achieve a certain level of performance, which we call \textit{label complexity} and express in \textit{Big-Oh} notation. An ideal active learner will compete with fully supervised learning with $|\mathcal{L}| \ll |\mathcal{S}|$. More realistically, we hope to at least place an upper bound on the error of active learning that is within a constant (multiplicative or additive) factor of the error of fully labeled supervised learning.

A popular approach to active learning when the problem is binary classification and realizable is a greedy strategy known as \textit{generalized binary search} (GBS), also called the \textit{splitting method}. Let $\mathcal{S}_t$ be the set of labeled data points after $t$ queries and $\mathcal{H}_t = \{ h : \hat{\epsilon}_{\mathcal{S}_t}(h) = 0 \mbox{ and } h \in \mathcal{H}\}$, i.e., the set of hypotheses from $\mathcal{H}$ that are consistent with the labeled data in $\mathcal{S}_t$. For the next ($t+1$) label query, we want to choose the unlabeled point that provokes the greatest disagreement between hypotheses in $\mathcal{H}_t$.The maximum disagreement occurs when half of the hypotheses predict one label and the rest the other. Equivalent, this minimizes the absolute value of the sum of all predicted labels: $| \sum_{h \in \mathcal{H}_t} h(x)|$ (when using $\pm1$ labels). Following this query policy, we can cut our current hypothesis space roughly in half with each query and achieve  a label complexity that is roughly $O(\log (d/\varepsilon))$ for $d$ the size (e.g., VC dimension) of the hypothesis space. Indeed, this approach yields an average case label complexity that is at most $\widetilde{O}(\log d)$ times larger than the optimal number of queries.



%
%to the theoretically
%
% An observation-response pair is sampled according some underlying (and nearly always unknown) distribution $(X, Y) \thicksim \mathcal{D}$. Oftentimes, we think of responses as deterministic, so that $Y = f(X)$. The learner's goal is to select a hypothesis that most closely 
%
%$h^\ast = \argmax
%
%
%\textit{Active learning} is an interactive sub-paradigm of supervised machine learning, in which a learning algorithm is given agency over the learning process. 
\subsection{Supervised learning theory}

In classic supervised machine learning, the learning algorithm (or \textit{learner}) is given the task of finding a response function $f: \mathcal{X} \mapsto \mathcal{Y}$ that predicts as accurately as possible the output \textit{response} $Y \in \mathcal{Y}$ for a given input observation	 $X \in \mathcal{X}$. Responses take a variety of forms. In classification, this may be a label from a discrete set of choices $\mathcal{Y} = \{ 1, 2, \dots\}$, while in regression it may be continuous. One of the most common tasks is binary classification, in which $\mathcal{Y} = \pm1$. We have some unknown underlying distribution $\mathcal{D}$ over the space of observations and responses $\mathcal{X} \times \mathcal{Y}$, so that observation-response pairs are sampled according to $(X, Y) \thicksim \mathcal{D}$. The learner chooses from candidate functions or \textit{hypotheses} in a hypothesis space $\mathcal{H}$ with the goal of minimizing the expected error or \textit{risk} $\epsilon_\mathcal{D}(h) = \mathbf{E}_{(X,Y)\thicksim \mathcal{D}}[\mathrm{err}(h(X), Y)]$. In other words, the learner's goal is to find $h^\ast$ that minimizes the risk: $h^\ast = \arg\max_{h \in \mathcal{H}} \epsilon_{\mathcal{D}}(h)$. For standard classification tasks, the error function is simply the indicator function of a mistake $\mathds{1}\{h(X) \not= Y\}$, and so the risk is simply the probability of a mistake $\epsilon(h) = \mathbf{E}_{(X,Y)\thicksim \mathcal{D}}[\mathds{1}\{h(X) \not= Y\}] = \mathbf{Pr}\{h(X) \not= Y\}$. For continuous response functions and multiclass classification where order matters, there are a wide choice of more complex error functions.

Of course, in practice $\mathcal{D}$ is unknown and so it is impossible to directly minimize the risk. Instead, the learner is provided with ``supervision'' in the form of a finite sample of observation-response pairs, i.e., a labeled \textit{training} data set $\mathcal{S} = \{ X_i, Y_i \}_{i=1, \dots, n}$ where $|\mathcal{S}| = n$. The learner can then approximate $\mathcal{D}$ using $\mathcal{S}$ and minimize the empirical error over $\mathcal{S}$:
\[
\hat{\epsilon}_{\mathcal{S}}(h) = \mathbf{E}_{(X,Y) \in \mathcal{S}}[\mathrm{err}(h(X), Y)] = \frac{1}{n} \sum_{i=1}^n \mathrm{err}(h(X_i), Y_i)
\]

\noindent Note that this definition of empirical risk assumes that samples $(X, Y)$ are identically independently distributed (IID), a fairly common assumption in supervised machine learning. In the \textit{empirical risk minimization} (ERM) paradigm, the learner assumes that the sample $\mathcal{S}$ is sufficiently representative of $\mathcal{D}$ such that choosing $\hat{h} = \arg\max_{h \in \mathcal{H}} \hat{\epsilon}_{\mathcal{S}}$ will yield a hypothesis $\hat{h}$ that will also have a relatively low risk $\epsilon_{\mathcal{D}}(\hat{h})$. A well known theoretical result for classification that comes from Vapnik tells us if we want to learn a ``good'' classifier from a hypothesis class $\mathcal{H}$, then we need roughly $|\mathcal{S}| = \widetilde{O}\left(d/\varepsilon^2 \log (1/\delta)\right)$ points in our training sample. Here $\varepsilon$ is the maximum deviation that we will tolerate between the true risks of $\hat{h}$ and optimal $h^\ast$ and $\delta$ is the probability with which we are willing to let this happen (i.e., we want $|\epsilon(\hat{h}) - \epsilon(h^\ast)| \leq \varepsilon$ to hold with probability $1-\delta$). Informally, $d$ represents the ``size'' of our hypothesis class; formally, it is the \textit{VC dimension}. A useful rule of thumb is that for most useful hypothesis classes, the VC dimension scales linearly with the number of parameters and so the number of training samples needed scales linearly with ``complexity'' of the model.

It is important to distinguish two cases of supervised learning, based on realizability. When the problem is \textit{realizable}, then there exists some hypothesis $h \in \mathcal{H}$ that can perfectly predict the response for every point (i.e., $\mathrm{err}(h^\ast) = 0$); in binary classification, this corresponds to the problem being ``separable'' by a hypothesis in $\mathcal{H}$. When $\mathrm{err}(h) > 0$, the problem is not realizable. The presence of \textit{label noise}, where the same point may receive different responses, further complicates this picture. If a training sample $\mathcal{S}$ contains noisy labels (perhaps due to error), this may mislead the ERM. If the true data distribution allows points to have different labels (i.e., our true labeling function is stochastic), then at best we may only be able to model $P(Y|X)$, rather than make perfect predictions.

\subsection{Selective sampling as a submodular problem}

Imagine the following problem, which we will call the \textbf{selective sampling on a budget} problem: given a large, fully labeled finite sample $\mathcal{S}$, we will ``purchase'' a subset $\mathcal{L} \subseteq \mathcal{S}$ (where $|\mathcal{L}| \ll |\mathcal{S}|$ because our ``cost'' scales with $|\mathcal{L}|$) and train $\bar{h} = \arg\max_{h \in \mathcal{H}} \hat{\epsilon}_{\mathcal{L}}(h)$ with the goal of minimizing $\epsilon_{\mathcal{D}}(\bar{h})$. We are given full access to $\mathcal{S}$ until we make our purchase decision, at which point we can use \textit{only} $\mathcal{L}$ to choose our final hypothesis (i.e., we must ``forget'' everything we know about $\mathcal{S} \setminus \mathcal{L}$). This can be thought of as choosing the smallest possible representative subsample $\mathcal{L}$. Intuitively, it is similar to a set cover problem: we want to pose queries that ``cover'' (i.e., eliminate) as many false hypotheses (inconsistent with our labeled data set) as possible. This problem clearly has submodular structure.\\

\begin{lemma}
The \textbf{selective sampling on a budget} problem is submodular and monotone decreasing.
\end{lemma}
\begin{proof}
We provide a non-rigorous justification. First, for labeled subsample $\mathcal{A}$, define a hypothesis set $\mathcal{H}_{\mathcal{A}} \subseteq \mathcal{H}$ that contains all hypotheses from $\mathcal{H}$ that are consistent with the labeled points in $\mathcal{A}$: $\mathcal{H}_{\mathcal{A}} = \{h : \hat{\epsilon}_{\mathcal{A}}(h)=0 \wedge h \in \mathcal{H}\}$. Now define a function $f(\mathcal{A}) = 1-|\mathcal{H}_{\mathcal{A}}| / |\mathcal{H}|$, i.e., maps $\mathcal{A}$ to the value of 1 minus probability mass (under a uniform prior) of its consistent hypothesis set. Now consider labeled subsamples $\mathcal{B}$ and $\mathcal{B}'$ such that $\mathcal{B} \subseteq \mathcal{B}' \subseteq \mathcal{S}$ and arbitrary point $X \not\in \mathcal{B}, \not\in \mathcal{B}'$. The key insight here is that as we add labeled points to our subsamples, we can only remove hypotheses from our current hypothesis space; once a hypothesis has been removed, it cannot be re-added.

\textbf{$f$ is monotone increasing:} Suppose that hypothesis $h \in \mathcal{H}_{\mathcal{B}'}$. This means that it is consistent with every labeled point in $\mathcal{B}'$. Because $\mathcal{B} \subseteq \mathcal{B}'$, $h$ must also be consistent with every point in $\mathcal{B}$ and so $h \in \mathcal{H}_{\mathcal{B}}$. Therefore, $\mathcal{H}_{\mathcal{B}'} \subseteq \mathcal{H}_{\mathcal{B}}$ and so

\begin{eqnarray*}
|\mathcal{H}_{\mathcal{B}'}| &\leq& |\mathcal{H}_{\mathcal{B}}| \\
|\mathcal{H}_{\mathcal{B}'}| / |\mathcal{H}| &\leq& |\mathcal{H}_{\mathcal{B}}| / |\mathcal{H}| \\
1-|\mathcal{H}_{\mathcal{B}'}| / |\mathcal{H}| &\geq& 1-|\mathcal{H}_{\mathcal{B}}| / |\mathcal{H}| \\
f(\mathcal{B}') &\geq& f(\mathcal{B})
\end{eqnarray*}

\noindent whenever $\mathcal{B} \subseteq \mathcal{B}'$.

\textbf{$f$ is submodular:} Now suppose that adding point $X$ to $\mathcal{B}'$ removes $h$ from $\mathcal{H}_{\mathcal{B}'}$, i.e., $h \in \mathcal{H}_{\mathcal{B}'} \setminus \mathcal{H}_{\mathcal{B}' \cup \{X\}}$. Because $\mathcal{H}_{\mathcal{B}'} \subseteq \mathcal{H}_{\mathcal{B}}$ whenever $\mathcal{B} \subseteq \mathcal{B}'$, it must also be the case that $\mathcal{H}_{\mathcal{B}' \cup \{X\}} \subseteq \mathcal{H}_{\mathcal{B} \cup \{X\}}$ and so $h \in \mathcal{H}_{\mathcal{B}} \setminus \mathcal{H}_{\mathcal{B} \cup \{X\}}$. Thus, if adding $X$ to $\mathcal{B}'$ removes $m$ hypotheses from $\mathcal{H}_{\mathcal{B}'}$, then adding $X$ to $\mathcal{B}$ must remove $n \geq m$ hypotheses from $\mathcal{H}_{\mathcal{B}}$.

\begin{eqnarray*}
m &\leq& n \\
|\mathcal{H}_{\mathcal{B}'}| - |\mathcal{H}_{\mathcal{B}'}| + m &\leq& |\mathcal{H}_{\mathcal{B}}| - |\mathcal{H}_{\mathcal{B}}| + n \\
|\mathcal{H}_{\mathcal{B}'}|/|\mathcal{H}| - (|\mathcal{H}_{\mathcal{B}'}| - m)/|\mathcal{H}| &\leq& |\mathcal{H}_{\mathcal{B}}|/|\mathcal{H}| - (|\mathcal{H}_{\mathcal{B}}| - n)/|\mathcal{H}| \\
-1 + |\mathcal{H}_{\mathcal{B}'}|/|\mathcal{H}| + 1 - (|\mathcal{H}_{\mathcal{B}'}| - m)/|\mathcal{H}| &\leq& -1 + |\mathcal{H}_{\mathcal{B}}|/|\mathcal{H}| + 1 - (|\mathcal{H}_{\mathcal{B}}| - n)/|\mathcal{H}| \\
1 - (|\mathcal{H}_{\mathcal{B}'}| - m)/|\mathcal{H}| - (1 - |\mathcal{H}_{\mathcal{B}'}|/|\mathcal{H}|) &\leq& 1 - (|\mathcal{H}_{\mathcal{B}}| - n)/|\mathcal{H}| - (1 - |\mathcal{H}_{\mathcal{B}}|/|\mathcal{H}|) \\
f(\mathcal{B}' \cup \{X\}) - f(\mathcal{B}') &\leq& f(\mathcal{B} \cup \{X\}) - f(\mathcal{B})
\end{eqnarray*}

\noindent whenever $\mathcal{B} \subseteq \mathcal{B}'$.
\end{proof}
\noindent This is intuitive. If $\mathcal{H}' \subseteq \mathcal{H}$ contains all hypotheses in $\mathcal{H}$ that are inconsistent with $X$'s label, then clearly $\mathcal{H}_{\mathcal{B}'} \subseteq \mathcal{H}_{\mathcal{B}}$ implies that $\mathcal{H}_{\mathcal{B}'} \setminus \mathcal{C} \subseteq \mathcal{H}_{\mathcal{B}} \setminus \mathcal{C}$.

This result may not seem terribly exciting, but what it does suggest is that we can solve the selective sampling on a budget problem using a greedy approach: on the $t$th iteration, choose the $X$ that eliminates the largest number of inconsistent hypotheses from our current $\mathcal{H}_{\mathcal{B}}$:
\[
(X, Y)_t = \underset{(X,Y) \in \mathcal{S} \setminus \mathcal{B}_t}{\arg\max} \left|\sum_{h \in \mathcal{H}_{\mathcal{B}_t}} \mathds{1}\{h(X) \not= Y\} \right|
\]

\subsection{Greedy active learning}

Now imagine a variation of the above selective sampling problem where we do not have access to the label of $X \in \mathcal{S}$ until we ``purchase'' it. Here we might use \textit{active learning}. Active learning is a variation of the supervised learning paradigm where the learner does not receive access to a fully labeled data sample $\mathcal{S}$ upfront. Rather it has access to an unlabeled data sample $\mathcal{U} = \{(X, ?)\}$, as well as an \textit{oracle} that the learner can \textit{query} for the response (or label) of an observation, $Y = \mathrm{or}(X)$. The active learner is given agency to choose which individual samples to label, but each query has a cost $c$ and the learner has only a limited \textit{budget} to spend on labeling data. Similar to the selective sampling scenario described above, the active learner has dual goals: to choose simultaneously a labeled subset of observations $\mathcal{L} \subseteq \mathcal{U}$ and a hypothesis $\bar{h} = \arg\min_{h \in \mathcal{H}} \hat{\epsilon}_{\mathcal{L}}(h)$ (i.e., $\bar{h}$ is the ERM for $\mathcal{L}$) that will yield the best possible predictive performance (i.e., lowest risk $\epsilon_{\mathcal{D}}(\bar{h}))$.

When evaluating active learning algorithms, we are concerned primarily with two performance properties: the quality (in terms of risk) of the hypotheses they produce and their query efficiency. Intuitively, a good active learner will use a very small number of label queries to produce a hypothesis with very small predictive error. More formally, we are interested in (1) how the error of the hypothesis produced by an active learner that chooses labeled subsample $\mathcal{L}$ compares with that of the hypothesis that we could learn from a fully labeled sample $\mathcal{S}$ where $\mathcal{L} \subseteq \mathcal{S}$; and (2) how many label queries must be made to achieve a certain level of performance, which we call \textit{label complexity} and express in \textit{Big-Oh} notation. An ideal active learner will compete with fully supervised learning with $|\mathcal{L}| \ll |\mathcal{S}|$. More realistically, we hope to at least place an upper bound on the error of active learning that is within a constant (multiplicative or additive) factor of the error of fully labeled supervised learning.

It is not hard to design greedy approaches to active learning, but two questions arise: first, are there greedy active learning algorithms that have sound theoretical guarantees about error and label complexity; and second, can we show that such algorithms are in fact specific cases of more general approaches based on submodularity? The answer to both of these questions is, in fact, yes. Let $\mathcal{S}_t$ be the set of labeled data points after $t$ queries (and recall that $\mathcal{H}_{\mathcal{S}_t}$ is the set of hypotheses from $\mathcal{H}$ consistent with the labeled data in $\mathcal{S}_t$). For the next ($t+1$) label query, we want to choose the unlabeled point that provokes the greatest disagreement between hypotheses in $\mathcal{H}_{\mathcal{S}_t}$.The maximum disagreement occurs when half of the hypotheses predict one label and the rest the other. Equivalently, this minimizes the absolute value of the sum of all predicted labels: $| \sum_{h \in \mathcal{H}_{\mathcal{S}_t}} h(x)|$ (when using $\pm1$ labels). Following this query policy, we hope to cut the $t$th hypothesis space roughly in half with query $t+1$ and achieve a label complexity that is roughly $O(\log (d/\varepsilon))$ for $d$ the size (e.g., VC dimension) of the hypothesis space. \cite{Dasgupta:2004} shows that in the worst case, this strategy may have to query every single label; indeed, for certain pathological cases, even the optimal query strategy will need to query ever label. However, the average case analysis is much more promising. On average, the greedy strategy's label complexity is at most $\widetilde{O}(\log d)$ times larger that of the optimal policy, as we show below in \ref{thm:dasgupta}, which rephrases \textit{Claim 4} and \textit{Theorem 3} from \cite{Dasgupta:2004}:\\

\begin{theorem}[Dasgupta ~\cite{Dasgupta:2004}]\label{thm:dasgupta}
Suppose the optimal query policy requires $m$ labels in expectation for target hypotheses chosen uniformly from hypothesis class $\mathcal{H}$ of (VC) dimension $d \geq e^e \approx 16$. Then the expected number of labels queried by the greedy strategy is at least $\frac{m \log d}{\log \log d}$ and at most $4 m \log d$.
\end{theorem}

As \cite{Dasgupta:2004} points out, the lower bound is a bit depressing, but we derive some comfort from the fact that the upper bound matches the lower bound within a multiplicative factor. We can extend this analysis to a Bayesian framework where we have a prior distribution over hypotheses $\pi(h)$. In this setting, we seek a label query that will divide the \textit{probability mass} over hypotheses (rather than the hypothesis space itself) in half. We do this by minimizing the absolute value of the sum of predictions weighted by the prior probabilities of the hypotheses making them: $| \sum_{h \in \mathcal{H}_t} \pi(x) h(x)|$. In this case, the $d$ term in the lower and upper bounds is replaced with $\min_{h \in \mathcal{H}} \pi(h)$.

\subsection{Greedy active learning as adaptive submodularity}


\subsection{Other active learning applications}
IN PROGRESS BUT FIRST, MUST SLEEP. If time/space, will get into detail. If not, will write this like a related work section.

\begin{itemize}
\item Bayesian experimental design
\item Distributed learning
\item Batch mode active learning
\end{itemize}

%
%to the theoretically
%
% An observation-response pair is sampled according some underlying (and nearly always unknown) distribution $(X, Y) \thicksim \mathcal{D}$. Oftentimes, we think of responses as deterministic, so that $Y = f(X)$. The learner's goal is to select a hypothesis that most closely 
%
%$h^\ast = \argmax
%
%
%\textit{Active learning} is an interactive sub-paradigm of supervised machine learning, in which a learning algorithm is given agency over the learning process. 
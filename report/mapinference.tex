In this section we will specifically look at the problem of Maximum a Posteriori Inference on graphs. To analyze the algorithms in greater detail, we would like to introduce a few preliminary notions, including the concept of Polymatroids. 
\subsubsection{Polymatroids}
The notion of submodularity was first studied in the context of matroids. A set system $(V,\mathcal{F})$ is defined by a ground set $V$ and a family of subsets $\mathcal{F} \subseteq 2^V$. Such a system is a matroid if
\begin{itemize}

\item $\emptyset \in \mathcal{F}$
\item $\text{ if } X \subseteq Y \in \mathcal{F} \text{ then } X\in \mathcal{F}$
\item $\text{ if } X,Y \in \mathcal{F} \text{ and } \|X\| > \|Y\| \text{ } \exists e\in X\setminus Y \text{ such that } Y+e\in\mathcal{F}$
\end{itemize}

Now we define a function $\rho$ called a rank function, which assings a natural number to each subset of $V$. This rank function is analagous to rank fuctions of matrices, in fact the matroid a which is the set of linearly independent columns of a matrix $A$ is called a metric matroid. We define our matroid {\it rank function} $\rho$ as follows
\[
\rho(X) = max\{\|F\| \text{ }\mid \text{ } F\in\mathcal{F}, \text{ }F\subseteq X\}
\]
If $\rho$ is a rank function of a matroid $(V,\mathcal{F})$ then the following properties hold:
\begin{itemize}
\item $\rho(X) \leq \|X\| \text{ }\forall X\subseteq Y$
\item $\rho \text{ is non decreasing: if } X \subseteq Y \subseteq V \text{ then } \rho(X) \leq \rho(Y)$
\item $\rho \text{ is submodular }$
\end{itemize}

If a set function $\rho$ satisfies the above properties for a ground set $V$ then the resulting structure $(V,\rho)$ is called a {\it polymatroid}. Similarly if $(V,\rho)$ is a polymatroid, then the family of subsets
\[
\mathcal{F} = \{ F\subseteq V \text{ }\mid \text{ } \rho(F) = \|F\|
\]
defines a matroid $(V,\mathcal{F})$.

\subsubsection{Cuts in Graphs, Energy Minimization and MAP Inference}
Consider a directed graph $G = (V,A,W)$ with positive edge weights $w:A\rightarrow\Re^+$. We can define a {\it positive directed cut} for a given set of vertices $S\subseteq V$ as the set of edges starting in $S$ and ending in $V\setminus S$ $:\delta^+(S) = \{(i,j)\in A\text{ }\mid \text{ }i \in S, j\in V\setminus S\}$, Similarly a negative directed cut is $\delta^-(S) = \{(i,j)\in A\text{ }\mid \text{ }i \in S, j\in V\setminus S\}$. Finally the cut $\delta(S) = \delta^+\cup\delta^-$, this for an undirected graph would be the set of edges with exactly one end in $S$. Hence we can define the weight of a cut as
\[
 f^+ = \underset{e\in\delta^+(S)}{\operatorname{\sum}} w(e), \text{ }  f^- = \underset{e\in\delta^-(S)}{\operatorname{\sum}} w(e), \text{ } f = \underset{e\in\delta^+(S)}{\operatorname{\sum}} w(e)
\]

Given these cut functions one can note that these cut functions are submodular.

\begin{itemize}
\item {\bf \lemma} {\it The cut functions $f^+$, $f^-$ and $f$ are submodular}\\
{\it Proof :} For the function $f$, suppose $X,Y\subset V$ then,
\[
f(X) + f(Y) - f(X\cup Y) - f(X\cap Y) = \underset{i\in\{X\setminus Y\}, j\in\{Y\setminus X\}}{\operatorname{\sum}} w(i,j) + \underset{i\in\{X\setminus Y\}, j\in\{Y\setminus X\}}{\operatorname{\sum}} w(j,i)
\]
from the non-negativity of edge weights we can quickly conclude the above function is submodular. Similarly submodularity can be proved for $f^+$ and $f^-$.
\end{itemize}
 Now in order to formalize our notion of Maximum a posteriori estimation as a submodular function minimization problem, we introduce the following notation. Consider the function $E:{0,1}^n\rightarrow\Re$ defined over binary variables $X={x_1,...,x_n}$. Such functions are called regular functions \cite{Kolmogorov04whatenergy}. We can define an equivalent set function $\hat{E}$
\[
\hat{E}(S) = E(x) \text{ where } x_i = 1 \text{ if and only if } i \in S
\]
We define the class $\mathcal{F}^2$ to be functions that can be written as a sum of functions of up to two binary variables at a time.
\[
E(x_1,....x_n) = \underset{i}{\operatorname{\sum}} E^i(x_i) + \underset{i<j}{\operatorname{\sum}} E^{i,j}(x_i,x_j)
\]
The regularity of the binary function from $\mathcal{F}^2$ translates to submodularity of the equivalent set function. 

Given an input set of nodes $\mathcal{P}$ in a graph $\mathcal{G}$ and a set of labels $\mathcal{L}$, the labeling $l$ (which is a mapping from $\mathcal{P}$ to $\mathcal{L}$) can be deduced by minimizing some energy function. In graph based energy minimization problems in computer vision and machine learning, the standard form of the energy function used is as follows
\[
E(l) = \underset{p\in\mathcal{P}}{\operatorname{\sum}} D_p(l_p) + \underset{p,q\in\mathcal{N}}{\operatorname{\sum}} V_{p,q}(l_p,l_q)
\]

where $\mathcal{N} \subset \mathcal{P}\times\mathcal{P}$ is a neighbourhood set of nodes. $D_p$ is a cost function derived from assigning label $l_p$ to node $p$. $V_{p,q}$ is the cost of assigning labels $l_p,l_q$ to adjacent nodes $p,q$. If V is a non-convex function of $\|l_p - l_q\|$ which accounts for border labeling, the energy function $E(l)$ is called a discontinuity preserving energy function. This label assignment problem is similar to the bears similarity to the graph cut problem. As the labeling function is submodular in the context of the {\it regular functions} defined earlier. Minimizing this energy function E is equivalent to finding the minimum cut of the graph $\mathcal{G}$. However one should note that the solution depends on the exact form of the function $V$ and it cannot be convex as it leads to oversmoothing of borders. If $V(l_p,l_q)=T[l_p\neq\l_q]$, where T is the indicator function. This smoothness term is called the Potts Model. The solution shown above can be readily extended to more that two labels, or beyond the binary problem. We use the binary problem to motivate the result. This result is widely used in computer vision in the domains of image segmentation , stereo correspondence and multi-camera image reconstruction. Another widely used application of this approach is to find the Maximum a posetiori estimate of a Markov Random Field \cite{MRFKohli}.

Consider a set of random variables $X = \{X_1,....,X_n\}$ defined on a set $S$ such that the variable $X_i$ can take the value $x_i$ from the set $\mathcal{L} = \{l_1,...l_n\}$. The $X$ can be defined as a Markov Random field with respect to the neighbourhood set $N = \{N_i \text{ } \mid \text{ } i\in$ iff, the positivity property $P(x) > 0$ and the Markovian property $P(x_i\mid x_{S\setminus {i}}) = P(x_i \mid x_{N_i}) \text{ } \forall i \in S$. Here $P(x) = P(X = x)$ and $P(x_i) = P(X_i = x_i)$ and finally $P(X_1 = x_1,...,X_n = x_n)  = (X = x) \text{ where } x = \{x_i \mid i\in S\}$ is a realization of the field. Given these definitions the MAP estimate of the MRF can be formulated as an energy minimization problem, where energy corresponding to a realization of x (configuration of the field) is given by the negative log likelihood of the joint posterior probability of the MRF
\[
\phi(x) = -logP(x\mid D)
\]
Hence the corresponding energy function for the Potts model becomes 
\[
E(x) =  \underset{i\in S}{\operatorname{\sum}} \left(\phi(D|x_i) + \underset{j\in \mathcal{N}_i}{\operatorname{\sum}}\psi{x_i,x_j} \right)
\]

where 
\[
\phi(D|x_i)  = -logP(i\in S)
\]
 and 
 \[
 \psi(x_i,x_j) = \left\{ 
  \begin{array}{l l}
    K_{ij} & \quad \text{if $x_i \neq x_j$}\\
    0 & \quad \text{if $x_i = x_j$}
  \end{array} \right.
 \]
 Here $K_{ij}$ is penalty some cost which makes $\psi(x_i,x_j)$ non convex.
 Hence it can be seen that {\it the energy minimization problem solved by min-cut, max flow which yields the minimum energy solution is equivalent to the maximium a posteriori solution of a Markov Random Field}.
 

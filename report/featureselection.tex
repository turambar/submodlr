In machine learning and statistics, \textit{feature selection} is one of the most important concepts. The aim of this process is to select a subset of relevant features for use in model construction and parameter fitting.  In real world problems, we often begin learning with a large number of candidate features that may be redundant, irrelevant, or noisy. Such features needlessly increase the complexity of our models and may lead to overfitting and poor generalization to previously unseen data. By pruning out redundant or irrelevant features, we can gain:
\begin{itemize}
\item improved model interpretability
\item increased computational efficiency, particularly during parameter fitting and prediction
\item enhanced generalization of the model by reducing overfitting
\end{itemize} 

In feature selection, we search among features and choose the ones that are, in a broad sense, most informative or useful. This definition can be interpreted as an optimization problem for choosing a subset of features which maximize the mutual information between features and labeling function.

Hence, if $V$ indicates the set of all features and a binary vector $S$ indicating the chosen feature set, and $x_S$ the real valued vector of feature values for features in $S$, then assuming that $||S||_1 \leq b$, we can write the problem as:
\begin{equation*}
max_s I(y;x_s)
\end{equation*} 
where $y$ is the labeling function.

\subsubsection{Submodularity}

We will now show that this is submodular. Suppose $A \subset B \subset S$ and $m \not \in B$. We can show that

\begin{eqnarray}
I(y;x_A \cup x_m) - I(y;x_A) &\geq & I(y;x_B \cup x_m) - I(y;x_B) \nonumber \\
\Leftrightarrow H(y|x_A)-H(y|x_A,x_m) &\geq & H(y|x_B)-H(y|x_B,x_m) \label{f1}
\end{eqnarray}
  
 we can write
\begin{eqnarray} 
	&&H(y|x_A)-H(y|x_A,x_m) \nonumber \\
	&=&H(y|x_A)+ H(x_m|x_A)-H(y,x_m|x_A)\nonumber \\
	&=&H(y|x_A)+ H(x_m|x_A) -H(y|x_A) - H(x_m|x_A,y)\nonumber \\
	&=& H(x_m|x_A)- H(x_m|x_A,y) \label{f2}
\end{eqnarray} 

By substituting into equation \ref{f1}, we can see that there are cases where this problem is not submodular. Here we give the necessary conditions for submodularity:

\begin{itemize}
\item {\bf \lemma} If $x_i$'s are all conditionally independent given y, then the function is submodular \cite{krausefeature}.
\end{itemize}

This constraint is met in many practical machine learning problems. If the $x_i$'s are all conditionally independent given $y$, then equation \ref{f2} can be written as

\begin{equation*}
H(y|x_A)-H(y|x_A,x_m)=H(x_m|x_A)-H(x_m|x_A,y)
\end{equation*}

and if we substitute this in equation \ref{f1}, then it follows that 

\begin{eqnarray*}
I(y;x_A \cup x_m) - I(y;x_A) &\geq & I(y;x_B \cup x_m) - I(y;x_B) \nonumber \\
\Leftrightarrow H(x_m|x_A) &\geq & H(x_m|x_B)
\end{eqnarray*} 

Hence, the problem of feature selection can be written as a maximization of a submodular function.\cite{jie}

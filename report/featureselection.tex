In machine learning and statistics, \textit{feature selection} is one of the most important concepts. The aim of this process is to select a subset of relevant features for use in model construction and parameter fitting.  In real world problems, we often begin learning with a large number of candidate features that may be redundant, irrelevant, or noisy. Such features needlessly increase the complexity of our models and may lead to overfitting and poor generalization to previously unseen data. By pruning out redundant or irrelevant features, we can gain:
\begin{itemize}
\item improved model interpretability
\item increased computational efficiency, particularly during parameter fitting and prediction
\item enhanced generalization of the model by reducing overfitting
\end{itemize} 

In feature selection, we search among features and choose the ones that are, in a broad sense, most informative or useful. This definition can be interpreted as an optimization problem for choosing a subset of features which maximize the mutual information between features and labeling function.

Hence, if $V$ indicates the set of all features and $S$ indicates the chosen feature set, and $x_S$ the vector of features in $S$, assuming that $||s||_1 \leq b$, we can write the problem as:
\begin{equation*}
max_s I(y;x_s)
\end{equation*} 
where $y$ is the labeling function.

\subsubsection{Submodularity}

Suppose $A \subset B \subset S$ and $m \not \in B$, for proving the submodularity, we should prove

\begin{eqnarray}
I(y;x_A \cup x_m) - I(y;x_A) &\geq & I(y;x_B \cup x_m) - I(y;x_B) \nonumber \\
\Leftrightarrow H(y|x_A)-H(y|x_A,x_m) &\geq & H(y|x_B)-H(y|x_B,x_m) \label{f1}
\end{eqnarray}
  
 we can write
\begin{eqnarray} 
	&&H(y|x_A)-H(y|x_A,x_m) \nonumber \\
	&=&H(y|x_A)+ H(x_m|x_A)-H(y,x_m|x_A)\nonumber \\
	&=&H(y|x_A)+ H(x_m|x_A) -H(y|x_A) - H(x_m|x_A,y)\nonumber \\
	&=& H(x_m|x_A)- H(x_m|x_A,y) \label{f2}
\end{eqnarray} 

 Substituting this term into equation \ref{f1}, it can be seen that there are cases in which the function is not submodular.
\begin{itemize}
\item {\bf \lemma} If $x_i$s are all conditionally independent given y, then the function is submodular \cite{krausefeature}.
\end{itemize}

This constraint is met in many practical problems in machine learning area. If $x_i$s are all conditionally independent given y, then equation \ref{f2} can be written as,
\begin{equation*}
H(y|x_A)-H(y|x_A,x_m)=H(x_m|x_A)-H(x_m|x_A,y)
\end{equation*}
and if we substitute this in equation \ref{f1},


\begin{eqnarray*}
I(y;x_A \cup x_m) - I(y;x_A) &\geq & I(y;x_B \cup x_m) - I(y;x_B) \nonumber \\
\Leftrightarrow H(x_m|x_A) &\geq & H(x_m|x_B)
\end{eqnarray*} 

Hence, the problem of feature selection can be written as a maximization of a submodular function.\cite{jie}
